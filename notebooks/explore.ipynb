{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-empire",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Load and check data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-prevention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('../data/sentences_with_sentiment.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-stopping",
   "metadata": {},
   "source": [
    "Drop the ID column since it contains no useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-prompt",
   "metadata": {},
   "source": [
    "Labels seem to be already one-hot encoded. Let's ensure the encoding is valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(data.loc[:, ['Positive', 'Negative', 'Neutral']].sum(axis=1) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-tracy",
   "metadata": {},
   "source": [
    "Check ```Sentence``` column for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300)\n",
    "dup = data['Sentence'].duplicated()\n",
    "dup[dup]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-commitment",
   "metadata": {},
   "source": [
    "Are the same rows also duplicates when considering also the labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_labels = data[['Sentence', 'Positive', 'Negative', 'Neutral']].duplicated()\n",
    "dup_labels[dup_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(dup[dup].index == dup_labels[dup_labels].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-celebration",
   "metadata": {},
   "source": [
    "Yes, it would seem so. \n",
    "\n",
    "In principle duplicated sentences could be used to represent opinions given by different experts, but since also the labels are the same this would not seem to be the case judging from this sample. The more likely explanation is that each duplicated value representes a common phrase that is _actually_ duplicated across various samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-mobile",
   "metadata": {},
   "source": [
    "Now we can check the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Positive samples', len(data[data['Positive'] == 1]))\n",
    "print('Negative samples', len(data[data['Negative'] == 1]))\n",
    "print('Neutral samples', len(data[data['Neutral'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-trader",
   "metadata": {},
   "source": [
    "The class distribution is clearly skewed towards positive sentiment. In addition, quite significant portion are neutral - this could be problematic since classifiers will probably have a hard time figuring out subtle differences.\n",
    "\n",
    "While were at it, lets produce a quick naive baseline for classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data['Positive'] == 1]) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-lloyd",
   "metadata": {},
   "source": [
    "By simply using the largest class as a prediction each time, we should expect on average 60 % accuracy (non-weighted). Any further classifiers should aim to at least outperform this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-theorem",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-diagnosis",
   "metadata": {},
   "source": [
    "### Unigram frequency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-mentor",
   "metadata": {},
   "source": [
    "Let's try to grasp some intuition behind data by listing out the most common words. The process involves building corpora of the sentences representing the three labels, filtering out knwon English language stop words and punctation, and finally counting the Frequency distributions amongst the indivudual corpora as well as the composite corpus. Throughout this process the excellent nltk library is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-pixel",
   "metadata": {},
   "source": [
    "Start by creating the corpora of Positive, Negative and Neutral labels respectively and tokenizing those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_pos = word_tokenize(' '.join(data.loc[data['Positive'] == 1, 'Sentence']).lower())\n",
    "corp_neg = word_tokenize(' '.join(data.loc[data['Negative'] == 1, 'Sentence']).lower())\n",
    "corp_neutr = word_tokenize(' '.join(data.loc[data['Neutral'] == 1, 'Sentence']).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_pos[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-savannah",
   "metadata": {},
   "source": [
    "Filter out known stopwords. Notice that before this operation stopwords need to be downloaded using:\n",
    "\n",
    "```\n",
    ">>> import nltk\n",
    ">>> nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "Then, a basis for a stop word list can be gotten from ```nltk.corpus.stopwords.words('english')```. Below, we will further tweak this basis list, in a attempt to reduce the noice present by meaningless words such as 'a', 'the', 'it' etc while still keeping acceptable discriminitive power between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = [\n",
    "    'i',\n",
    "    'me',\n",
    "    'my',\n",
    "    'myself',\n",
    "    'we',\n",
    "    'our',\n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'you',\n",
    "    \"you're\",\n",
    "    \"you've\",\n",
    "    \"you'll\",\n",
    "    \"you'd\",\n",
    "    'your',\n",
    "    'yours',\n",
    "    'yourself',\n",
    "    'yourselves',\n",
    "    'he',\n",
    "    'him',\n",
    "    'his',\n",
    "    'himself',\n",
    "    'she',\n",
    "    \"she's\",\n",
    "    'her',\n",
    "    'hers',\n",
    "    'herself',\n",
    "    'it',\n",
    "    \"it's\",\n",
    "    'its',\n",
    "    'itself',\n",
    "    'they',\n",
    "    'them',\n",
    "    'their',\n",
    "    'theirs',\n",
    "    'themselves',\n",
    "    'what',\n",
    "    'which',\n",
    "    'who',\n",
    "    'whom',\n",
    "    'this',\n",
    "    'that',\n",
    "    \"that'll\",\n",
    "    'these',\n",
    "    'those',\n",
    "    'am',\n",
    "    'is',\n",
    "    'are',\n",
    "    'was',\n",
    "    'were',\n",
    "    'be',\n",
    "    'been',\n",
    "    'being',\n",
    "    'have',\n",
    "    'has',\n",
    "    'had',\n",
    "    'having',\n",
    "    'do',\n",
    "    'does',\n",
    "    'did',\n",
    "    'doing',\n",
    "    'a',\n",
    "    'an',\n",
    "    'the',\n",
    "    'and',\n",
    "    'but',\n",
    "    'if',\n",
    "    'or',\n",
    "    'as',\n",
    "    'of',\n",
    "    'at',\n",
    "    'by',\n",
    "    'for',\n",
    "    'with',\n",
    "    'about',\n",
    "    'into',\n",
    "    'through',\n",
    "    'during',\n",
    "    'to',\n",
    "    'from',\n",
    "    'in',\n",
    "    'out',\n",
    "    'on',\n",
    "    'off',\n",
    "    'then',\n",
    "    'once',\n",
    "    'here',\n",
    "    'there',\n",
    "    'when',\n",
    "    'where',\n",
    "    'why',\n",
    "    'how',\n",
    "    'both',\n",
    "    'each',\n",
    "    'other',\n",
    "    'such',\n",
    "    'own',\n",
    "    'so',\n",
    "    's',\n",
    "    't',\n",
    "    'can',\n",
    "    'will',\n",
    "    'just',\n",
    "    'now',\n",
    "    'd',\n",
    "    'll',\n",
    "    'm',\n",
    "    'o',\n",
    "    're',\n",
    "    've',\n",
    "    'y',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_pos = [t for t in corp_pos if t not in sw]\n",
    "corp_neg = [t for t in corp_neg if t not in sw]\n",
    "corp_neutr = [t for t in corp_neutr if t not in sw]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-seafood",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_pos = [t for t in corp_pos if t not in string.punctuation]\n",
    "corp_neg = [t for t in corp_neg if t not in string.punctuation]\n",
    "corp_neutr = [t for t in corp_neutr if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-christianity",
   "metadata": {},
   "source": [
    "Then check out the freqdists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_pos = FreqDist(corp_pos)\n",
    "fd_neg = FreqDist(corp_neg)\n",
    "fd_neutr = FreqDist(corp_neutr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten = pd.DataFrame({\n",
    "    'Positive': [w[0] for w in fd_pos.most_common(10)],\n",
    "    'Pos_rate': [w[1] / len(data[data['Positive'] == 1]) for w in fd_pos.most_common(10)],\n",
    "    'Negative': [w[0] for w in fd_neg.most_common(10)],\n",
    "    'Neg_rate': [w[1] / len(data[data['Negative'] == 1]) for w in fd_neg.most_common(10)],    \n",
    "    'Neutral': [w[0] for w in fd_neutr.most_common(10)],\n",
    "    'Neutr_rate': [w[1] / len(data[data['Neutral'] == 1])for w in fd_neutr.most_common(10)]\n",
    "}, index=range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-screw",
   "metadata": {},
   "source": [
    "'Safety' and 'data' seem to be very popular words amongst both Positive and Negative corpora, although the proportions in the negative case are significantly higher. Words like 'should', 'further', 'limited' seem like obvious predictors for the negative class. In neutral class the word 'studies' is the most common ones, with 'safety' and 'data' receiving lower rankings. It is hence possible to hypothezise the following distiction:\n",
    "\n",
    "* Many negative and positive tend to be **argumentative** of why the given data does or does not show evidence of product safety. With safety concerns present, the authors tend to be more explicit in their wordings about 'data' and 'safety'\n",
    "* Neutral comments tend to be **descriptive** w.r.t. to the procedures followed during conducting and reporting the given study/studies\n",
    "\n",
    "This could prove to be an useful feature in one-vs-all classification approach. Obviously the dataset here is very limited, so the general applicability of these findings if of course questionable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-strand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(top_ten['Neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-appointment",
   "metadata": {},
   "source": [
    "### Bigram and trigram analysis\n",
    "\n",
    "A similar approach can be used for sequences of two and four words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_fd_pos = FreqDist(list(bigrams(corp_pos)))\n",
    "bi_fd_neg = FreqDist(list(bigrams(corp_neg)))\n",
    "bi_fd_neutr = FreqDist(list(bigrams(corp_neutr)))\n",
    "\n",
    "top_ten_bi = pd.DataFrame({\n",
    "    'Positive': [w[0] for w in bi_fd_pos.most_common(10)],\n",
    "    'Pos_rate': [w[1] / len(data[data['Positive'] == 1]) for w in bi_fd_pos.most_common(10)],\n",
    "    'Negative': [w[0] for w in bi_fd_neg.most_common(10)],\n",
    "    'Neg_rate': [w[1] / len(data[data['Negative'] == 1]) for w in bi_fd_neg.most_common(10)],    \n",
    "    'Neutral': [w[0] for w in bi_fd_neutr.most_common(10)],\n",
    "    'Neutr_rate': [w[1] / len(data[data['Neutral'] == 1])for w in bi_fd_neutr.most_common(10)]\n",
    "}, index=range(1,11))\n",
    "\n",
    "top_ten_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_fd_pos = FreqDist(list(trigrams(corp_pos)))\n",
    "tri_fd_neg = FreqDist(list(trigrams(corp_neg)))\n",
    "tri_fd_neutr = FreqDist(list(trigrams(corp_neutr)))\n",
    "\n",
    "top_ten_tri = pd.DataFrame({\n",
    "    'Positive': [w[0] for w in tri_fd_pos.most_common(10)],\n",
    "    'Pos_rate': [w[1] / len(data[data['Positive'] == 1]) for w in tri_fd_pos.most_common(10)],\n",
    "    'Negative': [w[0] for w in tri_fd_neg.most_common(10)],\n",
    "    'Neg_rate': [w[1] / len(data[data['Negative'] == 1]) for w in tri_fd_neg.most_common(10)],    \n",
    "    'Neutral': [w[0] for w in tri_fd_neutr.most_common(10)],\n",
    "    'Neutr_rate': [w[1] / len(data[data['Neutral'] == 1])for w in tri_fd_neutr.most_common(10)]\n",
    "}, index=range(1,11))\n",
    "\n",
    "top_ten_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-invalid",
   "metadata": {},
   "source": [
    "From these analyses it can be determined that there seem to be some phrases the evaluators frequently use word-for-word when describing limitations in the drug evaluation procedure. For instance, the phrase\n",
    "\n",
    "```chmp considers following measures```\n",
    "\n",
    "appears a total of four times (11 %) in the negative class, but not one single time in the positive class. \n",
    "\n",
    "From statistical point of view the dataset is probably too small to efficiently train on trigram-based features. Bigram features could offer some useful information, since at least the phrases 'safety profile' and 'clinical data' ore replicated in non-negligiable portion of Positive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-notebook",
   "metadata": {},
   "source": [
    "### Semantic lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-necklace",
   "metadata": {},
   "source": [
    "Semantic lexicon is a collection of words and phrases associated with a specific sentiment (Positive/Neutral/Negative). While there are some open source semantic lexicons available, best results could arguably be obtained by hand-curated lexicons. \n",
    "\n",
    "To demonstrate the concept, these phrases were manually gathered by examining the provided Excel-file. This is a 'poor man's semantic lexicon' in the sense that we only include the low-hanging fruits, i.e. phrases that clearly are repeated many times throughout the data. \n",
    "\n",
    "During the manual gathering process, the labels were hidden in order to get as objective as possible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = [\n",
    "    'based on the',\n",
    "    'bioequivalence',\n",
    "    'bioequivalent',\n",
    "    'biosimilarity',\n",
    "    'accepted by the chmp',\n",
    "    'comparable',\n",
    "    'these objectives have been met',\n",
    "    'the available safety data are considered supportive'\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    'should be provided',\n",
    "    'data are considered very limited',\n",
    "    'chmp considers the following measures',\n",
    "    \n",
    "]\n",
    "\n",
    "phrases = positives + negatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
